{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-McRreAcrFw4",
        "outputId": "a160acba-33fe-4f5f-e5d4-919cab4ee2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1001 entries, 0 to 1000\n",
            "Data columns (total 32 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   ï»¿issue_id        1000 non-null   float64\n",
            " 1   type               1000 non-null   object \n",
            " 2   status             1000 non-null   object \n",
            " 3   resolution         1000 non-null   object \n",
            " 4   component          1000 non-null   object \n",
            " 5   priority           1000 non-null   object \n",
            " 6   reporter           1000 non-null   object \n",
            " 7   created            1000 non-null   object \n",
            " 8   assigned           997 non-null    object \n",
            " 9   assignee           1000 non-null   object \n",
            " 10  resolved           1000 non-null   object \n",
            " 11  created.1          1000 non-null   float64\n",
            " 12  assigned.1         997 non-null    float64\n",
            " 13  summary            1000 non-null   object \n",
            " 14  description        1000 non-null   object \n",
            " 15  affected_version   1000 non-null   object \n",
            " 16  fixed_version      996 non-null    object \n",
            " 17  votes              1000 non-null   float64\n",
            " 18  watches            1000 non-null   float64\n",
            " 19  description_words  1000 non-null   float64\n",
            " 20  assingnee_count    1000 non-null   float64\n",
            " 21  comment_count      1000 non-null   float64\n",
            " 22  commenter          1000 non-null   float64\n",
            " 23  Surprising         1000 non-null   float64\n",
            " 24  Dormant            1000 non-null   float64\n",
            " 25  Blocker            1000 non-null   float64\n",
            " 26  Security           1000 non-null   float64\n",
            " 27  Performance        1000 non-null   float64\n",
            " 28  Breakage           1000 non-null   float64\n",
            " 29  commit_count       1000 non-null   float64\n",
            " 30  file_count         1000 non-null   float64\n",
            " 31  ï½iles            146 non-null    object \n",
            "dtypes: float64(17), object(15)\n",
            "memory usage: 250.4+ KB\n",
            "None\n",
            "\n",
            "First 5 rows:\n",
            "   ï»¿issue_id         type    status resolution  \\\n",
            "0         12.0  Improvement  Resolved      Fixed   \n",
            "1         46.0  Improvement  Resolved      Fixed   \n",
            "2         92.0          Bug  Resolved      Fixed   \n",
            "3        103.0  Improvement  Resolved      Fixed   \n",
            "4        192.0          Bug  Resolved      Fixed   \n",
            "\n",
            "                      component priority     reporter  \\\n",
            "0                                  Major    Eric Yang   \n",
            "1               ambari-agent       Major    Eric Yang   \n",
            "2               ambari-agent       Major    Eric Yang   \n",
            "3  ambari-server; ambari-web       Major    Eric Yang   \n",
            "4                                  Major  Ramya Sunil   \n",
            "\n",
            "                     created                   assigned     assignee  ...  \\\n",
            "0  2011/09/26 23:42:56 +0100  2011/09/26 23:42:56 +0100    Eric Yang  ...   \n",
            "1  2011/10/07 21:49:04 +0100  2011/10/07 21:49:04 +0100    Eric Yang  ...   \n",
            "2  2011/10/21 19:14:29 +0100  2011/10/21 19:14:29 +0100    Eric Yang  ...   \n",
            "3  2011/10/26 19:18:07 +0100  2011/10/26 19:18:07 +0100    Eric Yang  ...   \n",
            "4  2012/05/08 23:24:58 +0100  2012/05/08 23:24:58 +0100  Ramya Sunil  ...   \n",
            "\n",
            "  commenter  Surprising  Dormant Blocker Security Performance Breakage  \\\n",
            "0       1.0         0.0      0.0     0.0      0.0         0.0      0.0   \n",
            "1       1.0         0.0      0.0     0.0      0.0         0.0      0.0   \n",
            "2       0.0         0.0      0.0     0.0      0.0         0.0      0.0   \n",
            "3       0.0         0.0      0.0     0.0      0.0         0.0      0.0   \n",
            "4       2.0         1.0      0.0     0.0      0.0         0.0      0.0   \n",
            "\n",
            "   commit_count  file_count                                            ï½iles  \n",
            "0           0.0         0.0                                                NaN  \n",
            "1           0.0         0.0                                                NaN  \n",
            "2           0.0         0.0                                                NaN  \n",
            "3           0.0         0.0                                                NaN  \n",
            "4           1.0         2.0  CHANGES.txt;hmc/puppet/modules/hdp-hadoop/mani...  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "Cleaned and engineered dataset saved at: cleaned_engineered_dataset1.csv\n",
            "\n",
            "Sample of engineered features:\n",
            "   time_to_resolve  time_to_assign  time_to_fix  description_word_count  \\\n",
            "0         0.027778             0.0     0.022222                       0   \n",
            "1         0.028889             0.0     0.023111                       0   \n",
            "2         0.045278             0.0     0.036222                       0   \n",
            "3         0.121944             0.0     0.097556                       0   \n",
            "4         7.662222             0.0     6.129778                       0   \n",
            "\n",
            "   commenter_count  votes_watches_sum  priority_time_interaction  \n",
            "0                1                0.0                   0.055556  \n",
            "1                1                0.0                   0.057778  \n",
            "2                1                0.0                   0.090556  \n",
            "3                1                0.0                   0.243889  \n",
            "4                1                0.0                  15.324444  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = '/content/_ambari.csv'\n",
        "df = pd.read_csv(dataset_path, sep=',', encoding='ISO-8859-1', header=0)\n",
        "\n",
        "# Show dataset info and preview\n",
        "print('Dataset Info:')\n",
        "print(df.info())\n",
        "print('\\nFirst 5 rows:')\n",
        "print(df.head())\n",
        "\n",
        "# Handle missing values\n",
        "df.replace(['null', 'NULL', 'NaN', '', ' '], np.nan, inplace=True)\n",
        "\n",
        "# Fill missing numeric columns with median\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.median()))\n",
        "\n",
        "# Fill missing categorical columns with mode\n",
        "categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "df[categorical_cols] = df[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]))\n",
        "\n",
        "# Convert datetime columns\n",
        "datetime_cols = ['created', 'resolved', 'assigned']\n",
        "for col in datetime_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# Feature engineering\n",
        "\n",
        "# 1. Time to resolve (hours)\n",
        "df['time_to_resolve'] = (df['resolved'] - df['created']).dt.total_seconds() / 3600\n",
        "\n",
        "# 2. Time to assign (hours)\n",
        "if 'assigned' in df.columns:\n",
        "    df['time_to_assign'] = (df['assigned'] - df['created']).dt.total_seconds() / 3600\n",
        "    df['time_to_assign'] = df['time_to_assign'].fillna(0)\n",
        "\n",
        "# 3. Time to fix (80% of time to resolve)\n",
        "df['time_to_fix'] = df['time_to_resolve'] * 0.8\n",
        "\n",
        "# 4. Description word count\n",
        "if 'description' in df.columns:\n",
        "    df['description_word_count'] = df['description'].astype(str).str.split().str.len().fillna(0)\n",
        "else:\n",
        "    df['description_word_count'] = 0\n",
        "\n",
        "# 5. Number of commenters\n",
        "if 'commenter' in df.columns:\n",
        "    df['commenter_count'] = df['commenter'].astype(str).str.split(',').str.len().fillna(0)\n",
        "else:\n",
        "    df['commenter_count'] = 0\n",
        "\n",
        "# 6. Sum of votes and watches\n",
        "for col in ['votes', 'watches']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "df['votes_watches_sum'] = df[['votes', 'watches']].sum(axis=1)\n",
        "\n",
        "# 7. Binary flags for critical labels\n",
        "critical_labels = ['Surprising', 'Dormant', 'Blocker', 'Security', 'Performance', 'Breakage']\n",
        "for label in critical_labels:\n",
        "    if label in df.columns:\n",
        "        df[f'is_{label.lower()}'] = (df[label].notnull()).astype(int)\n",
        "    else:\n",
        "        df[f'is_{label.lower()}'] = 0\n",
        "\n",
        "# 8. Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_cols = ['type', 'status', 'resolution', 'component', 'priority', 'reporter', 'assignee']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "# 9. Interaction term: Priority * Time to resolve\n",
        "if 'priority' in df.columns:\n",
        "    df['priority_time_interaction'] = df['priority'] * df['time_to_resolve']\n",
        "else:\n",
        "    df['priority_time_interaction'] = 0\n",
        "\n",
        "# Save cleaned and engineered dataset\n",
        "cleaned_engineered_data_path = 'cleaned_engineered_dataset1.csv'\n",
        "df.to_csv(cleaned_engineered_data_path, index=False)\n",
        "print(f'Cleaned and engineered dataset saved at: {cleaned_engineered_data_path}')\n",
        "\n",
        "# Show sample of engineered features\n",
        "print('\\nSample of engineered features:')\n",
        "print(df[['time_to_resolve', 'time_to_assign', 'time_to_fix', 'description_word_count',\n",
        "          'commenter_count', 'votes_watches_sum', 'priority_time_interaction']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, BatchNormalization, Flatten, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = 'cleaned_engineered_dataset1.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Handling missing values (ensure no NaNs)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Convert categorical features to numerical using LabelEncoder\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "# Define target variable\n",
        "target = 'priority'\n",
        "y = df[target].values\n",
        "\n",
        "# Convert target to binary classification (e.g., median split)\n",
        "median_time = np.median(y)\n",
        "y_binary = (y >= median_time).astype(int)\n",
        "\n",
        "# Text processing for 'description' column (if available)\n",
        "if 'description' in df.columns:\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(df['description'].astype(str))\n",
        "    X_text = tokenizer.texts_to_sequences(df['description'].astype(str))\n",
        "    X_text = pad_sequences(X_text, maxlen=100)\n",
        "else:\n",
        "    X_text = np.zeros((len(df), 100))\n",
        "\n",
        "# Extract numerical and categorical features\n",
        "excluded_cols = ['description', target]\n",
        "feature_cols = [col for col in df.columns if col not in excluded_cols]\n",
        "X_tabular = df[feature_cols].values\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_tabular = scaler.fit_transform(X_tabular)\n",
        "\n",
        "# Split dataset\n",
        "X_text_train, X_text_test, X_tab_train, X_tab_test, y_train, y_test = train_test_split(\n",
        "    X_text, X_tabular, y_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model architecture\n",
        "text_input = Input(shape=(100,), name='text_input')\n",
        "text_embedding = Embedding(input_dim=5000, output_dim=128, input_length=100)(text_input)\n",
        "text_lstm = LSTM(64, return_sequences=False)(text_embedding)\n",
        "text_dense = Dense(32, activation='relu')(text_lstm)\n",
        "\n",
        "# Tabular input\n",
        "tabular_input = Input(shape=(X_tab_train.shape[1],), name='tabular_input')\n",
        "tabular_dense = Dense(64, activation='relu')(tabular_input)\n",
        "tabular_dense = BatchNormalization()(tabular_dense)\n",
        "tabular_dense = Dropout(0.3)(tabular_dense)\n",
        "\n",
        "# Combine both inputs\n",
        "merged = Concatenate()([text_dense, tabular_dense])\n",
        "final_dense = Dense(64, activation='relu')(merged)\n",
        "final_dense = Dropout(0.3)(final_dense)\n",
        "output = Dense(1, activation='sigmoid', name='output')(final_dense)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=[text_input, tabular_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(\n",
        "    [X_text_train, X_tab_train], y_train,\n",
        "    validation_data=([X_text_test, X_tab_test], y_test),\n",
        "    epochs=20, batch_size=32\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model.save('/content/rnn_mlp_model1.h5')\n",
        "print('Model saved!')\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_probs = model.predict([X_text_test, X_tab_test])\n",
        "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred_probs)\n",
        "mae = mean_absolute_error(y_test, y_pred_probs)\n",
        "r2 = r2_score(y_test, y_pred_probs)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'R^2 Score: {r2}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmHK90HdEQFM",
        "outputId": "861d1d59-b9e2-4655-a0fe-0e22ec3cced4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.4799 - loss: 0.9752 - val_accuracy: 0.7910 - val_loss: 0.6436\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8811 - loss: 0.3598 - val_accuracy: 0.7910 - val_loss: 0.5031\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8635 - loss: 0.3761 - val_accuracy: 0.7910 - val_loss: 0.5323\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8815 - loss: 0.3528 - val_accuracy: 0.7910 - val_loss: 0.5203\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9017 - loss: 0.3331 - val_accuracy: 0.7910 - val_loss: 0.4942\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8903 - loss: 0.3088 - val_accuracy: 0.7910 - val_loss: 0.4895\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8664 - loss: 0.3678 - val_accuracy: 0.7910 - val_loss: 0.5213\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8949 - loss: 0.3058 - val_accuracy: 0.7910 - val_loss: 0.4686\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8738 - loss: 0.3299 - val_accuracy: 0.7910 - val_loss: 0.5175\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8742 - loss: 0.3549 - val_accuracy: 0.7910 - val_loss: 0.4838\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8694 - loss: 0.3252 - val_accuracy: 0.7910 - val_loss: 0.4815\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8903 - loss: 0.2996 - val_accuracy: 0.7910 - val_loss: 0.4803\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8750 - loss: 0.3334 - val_accuracy: 0.7910 - val_loss: 0.4978\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8988 - loss: 0.2738 - val_accuracy: 0.7910 - val_loss: 0.4734\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8967 - loss: 0.2914 - val_accuracy: 0.7910 - val_loss: 0.4622\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8959 - loss: 0.2929 - val_accuracy: 0.7910 - val_loss: 0.4742\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8940 - loss: 0.2733 - val_accuracy: 0.7861 - val_loss: 0.4737\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8838 - loss: 0.2810 - val_accuracy: 0.7910 - val_loss: 0.4810\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9006 - loss: 0.2597 - val_accuracy: 0.7910 - val_loss: 0.4556\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8871 - loss: 0.2986 - val_accuracy: 0.7861 - val_loss: 0.4828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Mean Squared Error: 0.1532512903213501\n",
            "Mean Absolute Error: 0.23561657965183258\n",
            "R^2 Score: 0.07285046577453613\n",
            "Accuracy: 0.7860696517412935\n",
            "Precision: 0.79\n",
            "Recall: 0.9937106918238994\n",
            "F1 Score: 0.8802228412256268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = '/content/_camel.csv'\n",
        "df = pd.read_csv(dataset_path, sep=',', encoding='ISO-8859-1', header=0)\n",
        "\n",
        "# Show dataset info and preview\n",
        "print('Dataset Info:')\n",
        "print(df.info())\n",
        "print('\\nFirst 5 rows:')\n",
        "print(df.head())\n",
        "\n",
        "# Handle missing values\n",
        "df.replace(['null', 'NULL', 'NaN', '', ' '], np.nan, inplace=True)\n",
        "\n",
        "# Fill missing numeric columns with median\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.median()))\n",
        "\n",
        "# Fill missing categorical columns with mode\n",
        "categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "df[categorical_cols] = df[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]))\n",
        "\n",
        "# Convert datetime columns\n",
        "datetime_cols = ['created', 'resolved', 'assigned']\n",
        "for col in datetime_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# Feature engineering\n",
        "\n",
        "# 1. Time to resolve (hours)\n",
        "df['time_to_resolve'] = (df['resolved'] - df['created']).dt.total_seconds() / 3600\n",
        "\n",
        "# 2. Time to assign (hours)\n",
        "if 'assigned' in df.columns:\n",
        "    df['time_to_assign'] = (df['assigned'] - df['created']).dt.total_seconds() / 3600\n",
        "    df['time_to_assign'] = df['time_to_assign'].fillna(0)\n",
        "\n",
        "# 3. Time to fix (80% of time to resolve)\n",
        "df['time_to_fix'] = df['time_to_resolve'] * 0.8\n",
        "\n",
        "# 4. Description word count\n",
        "if 'description' in df.columns:\n",
        "    df['description_word_count'] = df['description'].astype(str).str.split().str.len().fillna(0)\n",
        "else:\n",
        "    df['description_word_count'] = 0\n",
        "\n",
        "# 5. Number of commenters\n",
        "if 'commenter' in df.columns:\n",
        "    df['commenter_count'] = df['commenter'].astype(str).str.split(',').str.len().fillna(0)\n",
        "else:\n",
        "    df['commenter_count'] = 0\n",
        "\n",
        "# 6. Sum of votes and watches\n",
        "for col in ['votes', 'watches']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "df['votes_watches_sum'] = df[['votes', 'watches']].sum(axis=1)\n",
        "\n",
        "# 7. Binary flags for critical labels\n",
        "critical_labels = ['Surprising', 'Dormant', 'Blocker', 'Security', 'Performance', 'Breakage']\n",
        "for label in critical_labels:\n",
        "    if label in df.columns:\n",
        "        df[f'is_{label.lower()}'] = (df[label].notnull()).astype(int)\n",
        "    else:\n",
        "        df[f'is_{label.lower()}'] = 0\n",
        "\n",
        "# 8. Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_cols = ['type', 'status', 'resolution', 'component', 'priority', 'reporter', 'assignee']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "# 9. Interaction term: Priority * Time to resolve\n",
        "if 'priority' in df.columns:\n",
        "    df['priority_time_interaction'] = df['priority'] * df['time_to_resolve']\n",
        "else:\n",
        "    df['priority_time_interaction'] = 0\n",
        "\n",
        "# Save cleaned and engineered dataset\n",
        "cleaned_engineered_data_path = 'cleaned_engineered_dataset2.csv'\n",
        "df.to_csv(cleaned_engineered_data_path, index=False)\n",
        "print(f'Cleaned and engineered dataset saved at: {cleaned_engineered_data_path}')\n",
        "\n",
        "# Show sample of engineered features\n",
        "print('\\nSample of engineered features:')\n",
        "print(df[['time_to_resolve', 'time_to_assign', 'time_to_fix', 'description_word_count',\n",
        "          'commenter_count', 'votes_watches_sum', 'priority_time_interaction']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGfU6orcn5mi",
        "outputId": "5b38682e-5917-425b-9eca-90c8adf54dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 32 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   ï»¿issue_id        1000 non-null   int64  \n",
            " 1   type               1000 non-null   object \n",
            " 2   status             1000 non-null   object \n",
            " 3   resolution         1000 non-null   object \n",
            " 4   component          877 non-null    object \n",
            " 5   priority           1000 non-null   object \n",
            " 6   reporter           1000 non-null   object \n",
            " 7   created            1000 non-null   object \n",
            " 8   assigned           977 non-null    object \n",
            " 9   assignee           1000 non-null   object \n",
            " 10  resolved           1000 non-null   object \n",
            " 11  created.1          1000 non-null   float64\n",
            " 12  assigned.1         977 non-null    float64\n",
            " 13  summary            1000 non-null   object \n",
            " 14  description        1000 non-null   object \n",
            " 15  affected_version   1000 non-null   object \n",
            " 16  fixed_version      997 non-null    object \n",
            " 17  votes              1000 non-null   int64  \n",
            " 18  watches            1000 non-null   int64  \n",
            " 19  description_words  1000 non-null   int64  \n",
            " 20  assingnee_count    1000 non-null   int64  \n",
            " 21  comment_count      1000 non-null   int64  \n",
            " 22  commenter          1000 non-null   int64  \n",
            " 23  Surprising         1000 non-null   int64  \n",
            " 24  Dormant            1000 non-null   int64  \n",
            " 25  Blocker            999 non-null    object \n",
            " 26  Security           999 non-null    float64\n",
            " 27  Performance        999 non-null    float64\n",
            " 28  Breakage           999 non-null    float64\n",
            " 29  commit_count       999 non-null    float64\n",
            " 30  file_count         999 non-null    float64\n",
            " 31  ï½iles            959 non-null    object \n",
            "dtypes: float64(7), int64(9), object(16)\n",
            "memory usage: 250.1+ KB\n",
            "None\n",
            "\n",
            "First 5 rows:\n",
            "   ï»¿issue_id         type  status resolution       component priority  \\\n",
            "0           72          Bug  Closed      Fixed      camel-core    Major   \n",
            "1           84  Improvement  Closed      Fixed      camel-core    Major   \n",
            "2          100  Improvement  Closed      Fixed  camel-activemq    Minor   \n",
            "3          112          Bug  Closed      Fixed  camel-activemq    Major   \n",
            "4          132  Improvement  Closed      Fixed      camel-http    Major   \n",
            "\n",
            "              reporter                    created                   assigned  \\\n",
            "0         Willem Jiang  2007/07/08 11:33:06 +0100                        NaN   \n",
            "1    Brian McCallister  2007/08/03 00:41:31 +0100  2008/05/21 18:50:34 +0100   \n",
            "2  Aaron Crickenberger  2007/08/13 20:28:46 +0100                        NaN   \n",
            "3       james strachan  2007/08/17 17:05:54 +0100                        NaN   \n",
            "4       james strachan  2007/08/31 12:12:02 +0100                        NaN   \n",
            "\n",
            "          assignee  ... commenter  Surprising  Dormant Blocker Security  \\\n",
            "0       Unassigned  ...         1           0        0       0      0.0   \n",
            "1  Hadrian Zbarcea  ...         3           0        0       0      0.0   \n",
            "2       Unassigned  ...         2           0        0       0      0.0   \n",
            "3       Unassigned  ...         1           0        0       0      0.0   \n",
            "4       Unassigned  ...         1           1        0       0      0.0   \n",
            "\n",
            "  Performance Breakage  commit_count  file_count  \\\n",
            "0         0.0      0.0           1.0         1.0   \n",
            "1         0.0      0.0           2.0         2.0   \n",
            "2         0.0      1.0           3.0        11.0   \n",
            "3         0.0      0.0           1.0         4.0   \n",
            "4         0.0      0.0           1.0         4.0   \n",
            "\n",
            "                                             ï½iles  \n",
            "0  camel-core/src/test/java/org/apache/camel/comp...  \n",
            "1  camel-core/src/main/java/org/apache/camel/NoTy...  \n",
            "2  tests/camel-partial-classpath-test/pom.xml;tes...  \n",
            "3  components/camel-activemq/src/test/java/org/ap...  \n",
            "4  components/camel-http/src/main/java/org/apache...  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "Cleaned and engineered dataset saved at: cleaned_engineered_dataset2.csv\n",
            "\n",
            "Sample of engineered features:\n",
            "   time_to_resolve  time_to_assign  time_to_fix  description_word_count  \\\n",
            "0        22.453611     1961.527778    17.962889                       0   \n",
            "1     10071.810556     7026.150833  8057.448444                       0   \n",
            "2       518.604167     1088.600000   414.883333                       0   \n",
            "3         0.121667      995.981111     0.097333                       0   \n",
            "4         0.521944      664.878889     0.417556                       0   \n",
            "\n",
            "   commenter_count  votes_watches_sum  priority_time_interaction  \n",
            "0                1                  0                  44.907222  \n",
            "1                1                  0               20143.621111  \n",
            "2                1                  0                1555.812500  \n",
            "3                1                  0                   0.243333  \n",
            "4                1                  0                   1.043889  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, BatchNormalization, Flatten, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = 'cleaned_engineered_dataset2.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Handling missing values (ensure no NaNs)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Convert categorical features to numerical using LabelEncoder\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "# Define target variable\n",
        "target = 'priority'\n",
        "y = df[target].values\n",
        "\n",
        "# Convert target to binary classification (e.g., median split)\n",
        "median_time = np.median(y)\n",
        "y_binary = (y >= median_time).astype(int)\n",
        "\n",
        "# Text processing for 'description' column (if available)\n",
        "if 'description' in df.columns:\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(df['description'].astype(str))\n",
        "    X_text = tokenizer.texts_to_sequences(df['description'].astype(str))\n",
        "    X_text = pad_sequences(X_text, maxlen=100)\n",
        "else:\n",
        "    X_text = np.zeros((len(df), 100))\n",
        "\n",
        "# Extract numerical and categorical features\n",
        "excluded_cols = ['description', target]\n",
        "feature_cols = [col for col in df.columns if col not in excluded_cols]\n",
        "X_tabular = df[feature_cols].values\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_tabular = scaler.fit_transform(X_tabular)\n",
        "\n",
        "# Split dataset\n",
        "X_text_train, X_text_test, X_tab_train, X_tab_test, y_train, y_test = train_test_split(\n",
        "    X_text, X_tabular, y_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model architecture\n",
        "text_input = Input(shape=(100,), name='text_input')\n",
        "text_embedding = Embedding(input_dim=5000, output_dim=128, input_length=100)(text_input)\n",
        "text_lstm = LSTM(64, return_sequences=False)(text_embedding)\n",
        "text_dense = Dense(32, activation='relu')(text_lstm)\n",
        "\n",
        "# Tabular input\n",
        "tabular_input = Input(shape=(X_tab_train.shape[1],), name='tabular_input')\n",
        "tabular_dense = Dense(64, activation='relu')(tabular_input)\n",
        "tabular_dense = BatchNormalization()(tabular_dense)\n",
        "tabular_dense = Dropout(0.3)(tabular_dense)\n",
        "\n",
        "# Combine both inputs\n",
        "merged = Concatenate()([text_dense, tabular_dense])\n",
        "final_dense = Dense(64, activation='relu')(merged)\n",
        "final_dense = Dropout(0.3)(final_dense)\n",
        "output = Dense(1, activation='sigmoid', name='output')(final_dense)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=[text_input, tabular_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(\n",
        "    [X_text_train, X_tab_train], y_train,\n",
        "    validation_data=([X_text_test, X_tab_test], y_test),\n",
        "    epochs=20, batch_size=32\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model.save('/content/rnn_mlp_model1.h5')\n",
        "print('Model saved!')\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_probs = model.predict([X_text_test, X_tab_test])\n",
        "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred_probs)\n",
        "mae = mean_absolute_error(y_test, y_pred_probs)\n",
        "r2 = r2_score(y_test, y_pred_probs)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'R^2 Score: {r2}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RP4qPqqomsm",
        "outputId": "8ee30ddf-0e03-4824-eb63-0978b9f07599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.4799 - loss: 0.9161 - val_accuracy: 0.9950 - val_loss: 0.0772\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9568 - loss: 0.1947 - val_accuracy: 0.9950 - val_loss: 0.0371\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9710 - loss: 0.1274 - val_accuracy: 0.9950 - val_loss: 0.0473\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9620 - loss: 0.1611 - val_accuracy: 0.9950 - val_loss: 0.0469\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9720 - loss: 0.1264 - val_accuracy: 0.9950 - val_loss: 0.0428\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9667 - loss: 0.1261 - val_accuracy: 0.9950 - val_loss: 0.0443\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9712 - loss: 0.1208 - val_accuracy: 0.9950 - val_loss: 0.0489\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9751 - loss: 0.1020 - val_accuracy: 0.9950 - val_loss: 0.0464\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9729 - loss: 0.1041 - val_accuracy: 0.9950 - val_loss: 0.0456\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9588 - loss: 0.1320 - val_accuracy: 0.9950 - val_loss: 0.0411\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9546 - loss: 0.1404 - val_accuracy: 0.9950 - val_loss: 0.0404\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9689 - loss: 0.1159 - val_accuracy: 0.9950 - val_loss: 0.0432\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9700 - loss: 0.0995 - val_accuracy: 0.9950 - val_loss: 0.0391\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9562 - loss: 0.1437 - val_accuracy: 0.9950 - val_loss: 0.0396\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9782 - loss: 0.0785 - val_accuracy: 0.9950 - val_loss: 0.0423\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9703 - loss: 0.1005 - val_accuracy: 0.9950 - val_loss: 0.0444\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9676 - loss: 0.1089 - val_accuracy: 0.9950 - val_loss: 0.0417\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9736 - loss: 0.0911 - val_accuracy: 0.9950 - val_loss: 0.0464\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9773 - loss: 0.0755 - val_accuracy: 0.9950 - val_loss: 0.0456\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9628 - loss: 0.1112 - val_accuracy: 0.9950 - val_loss: 0.0425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Mean Squared Error: 0.006395124364644289\n",
            "Mean Absolute Error: 0.022321077063679695\n",
            "R^2 Score: -0.28545236587524414\n",
            "Accuracy: 0.995\n",
            "Precision: 0.995\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9974937343358395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = '/content/_derby.csv'\n",
        "df = pd.read_csv(dataset_path, sep=',', encoding='ISO-8859-1', header=0)\n",
        "\n",
        "# Show dataset info and preview\n",
        "print('Dataset Info:')\n",
        "print(df.info())\n",
        "print('\\nFirst 5 rows:')\n",
        "print(df.head())\n",
        "\n",
        "# Handle missing values\n",
        "df.replace(['null', 'NULL', 'NaN', '', ' '], np.nan, inplace=True)\n",
        "\n",
        "# Fill missing numeric columns with median\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.median()))\n",
        "\n",
        "# Fill missing categorical columns with mode\n",
        "categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "df[categorical_cols] = df[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]))\n",
        "\n",
        "# Convert datetime columns\n",
        "datetime_cols = ['created', 'resolved', 'assigned']\n",
        "for col in datetime_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# Feature engineering\n",
        "\n",
        "# 1. Time to resolve (hours)\n",
        "df['time_to_resolve'] = (df['resolved'] - df['created']).dt.total_seconds() / 3600\n",
        "\n",
        "# 2. Time to assign (hours)\n",
        "if 'assigned' in df.columns:\n",
        "    df['time_to_assign'] = (df['assigned'] - df['created']).dt.total_seconds() / 3600\n",
        "    df['time_to_assign'] = df['time_to_assign'].fillna(0)\n",
        "\n",
        "# 3. Time to fix (80% of time to resolve)\n",
        "df['time_to_fix'] = df['time_to_resolve'] * 0.8\n",
        "\n",
        "# 4. Description word count\n",
        "if 'description' in df.columns:\n",
        "    df['description_word_count'] = df['description'].astype(str).str.split().str.len().fillna(0)\n",
        "else:\n",
        "    df['description_word_count'] = 0\n",
        "\n",
        "# 5. Number of commenters\n",
        "if 'commenter' in df.columns:\n",
        "    df['commenter_count'] = df['commenter'].astype(str).str.split(',').str.len().fillna(0)\n",
        "else:\n",
        "    df['commenter_count'] = 0\n",
        "\n",
        "# 6. Sum of votes and watches\n",
        "for col in ['votes', 'watches']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "df['votes_watches_sum'] = df[['votes', 'watches']].sum(axis=1)\n",
        "\n",
        "# 7. Binary flags for critical labels\n",
        "critical_labels = ['Surprising', 'Dormant', 'Blocker', 'Security', 'Performance', 'Breakage']\n",
        "for label in critical_labels:\n",
        "    if label in df.columns:\n",
        "        df[f'is_{label.lower()}'] = (df[label].notnull()).astype(int)\n",
        "    else:\n",
        "        df[f'is_{label.lower()}'] = 0\n",
        "\n",
        "# 8. Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_cols = ['type', 'status', 'resolution', 'component', 'priority', 'reporter', 'assignee']\n",
        "for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "# 9. Interaction term: Priority * Time to resolve\n",
        "if 'priority' in df.columns:\n",
        "    df['priority_time_interaction'] = df['priority'] * df['time_to_resolve']\n",
        "else:\n",
        "    df['priority_time_interaction'] = 0\n",
        "\n",
        "# Save cleaned and engineered dataset\n",
        "cleaned_engineered_data_path = 'cleaned_engineered_dataset3.csv'\n",
        "df.to_csv(cleaned_engineered_data_path, index=False)\n",
        "print(f'Cleaned and engineered dataset saved at: {cleaned_engineered_data_path}')\n",
        "\n",
        "# Show sample of engineered features\n",
        "print('\\nSample of engineered features:')\n",
        "print(df[['time_to_resolve', 'time_to_assign', 'time_to_fix', 'description_word_count',\n",
        "          'commenter_count', 'votes_watches_sum', 'priority_time_interaction']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJQJ8Vt0o9Sc",
        "outputId": "6fdb96f7-4b7f-4ffe-9228-d9361d3a83c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 36 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   ï»¿IssueId                1000 non-null   int64  \n",
            " 1   type                      1000 non-null   object \n",
            " 2   status                    1000 non-null   object \n",
            " 3   resolution                1000 non-null   object \n",
            " 4   component                 989 non-null    object \n",
            " 5   priority                  1000 non-null   object \n",
            " 6   reporter                  1000 non-null   object \n",
            " 7   created                   1000 non-null   object \n",
            " 8   assigned                  972 non-null    object \n",
            " 9   assignee                  1000 non-null   object \n",
            " 10  resolved                  1000 non-null   object \n",
            " 11  fixingTimeFromCreate      1000 non-null   float64\n",
            " 12  fixingTimeFromAssignment  972 non-null    object \n",
            " 13  summary                   1000 non-null   object \n",
            " 14  description               1000 non-null   object \n",
            " 15  affectVersion             999 non-null    object \n",
            " 16  fixVersion                993 non-null    object \n",
            " 17  votes                     1000 non-null   int64  \n",
            " 18  watches                   1000 non-null   int64  \n",
            " 19  description_words         1000 non-null   int64  \n",
            " 20  assingnee_count           1000 non-null   int64  \n",
            " 21  comment_count             1000 non-null   int64  \n",
            " 22  commentator_count         1000 non-null   int64  \n",
            " 23  Surprising                1000 non-null   int64  \n",
            " 24  Dormant                   1000 non-null   int64  \n",
            " 25  Blocker                   1000 non-null   int64  \n",
            " 26  Security                  1000 non-null   int64  \n",
            " 27  Performance               1000 non-null   int64  \n",
            " 28  Breakage                  1000 non-null   int64  \n",
            " 29  commit_count              1000 non-null   int64  \n",
            " 30  file_count                1000 non-null   object \n",
            " 31  Files                     991 non-null    object \n",
            " 32  Unnamed: 32               0 non-null      float64\n",
            " 33  Unnamed: 33               0 non-null      float64\n",
            " 34  Unnamed: 34               0 non-null      float64\n",
            " 35  Unnamed: 35               0 non-null      float64\n",
            "dtypes: float64(5), int64(14), object(17)\n",
            "memory usage: 281.4+ KB\n",
            "None\n",
            "\n",
            "First 5 rows:\n",
            "   ï»¿IssueId         type  status resolution       component priority  \\\n",
            "0           5          Bug  Closed      Fixed  Network Server    Major   \n",
            "1          19          Bug  Closed      Fixed        Services    Minor   \n",
            "2          38          Bug  Closed      Fixed             SQL    Major   \n",
            "3          42  Improvement  Closed      Fixed        Services    Major   \n",
            "4          44  Improvement  Closed      Fixed            JDBC    Major   \n",
            "\n",
            "              reporter                    created                   assigned  \\\n",
            "0       Tulika Agrawal  2004/09/28 01:20:46 +0100  2004/09/28 01:20:46 +0100   \n",
            "1       Ramandeep Kaur  2004/09/29 20:09:06 +0100  2005/05/26 06:25:28 +0100   \n",
            "2      Mamta A. Satoor  2004/10/11 05:47:29 +0100  2004/10/13 15:52:37 +0100   \n",
            "3  Sunitha Kambhampati  2004/10/15 01:26:43 +0100  2004/10/15 01:26:43 +0100   \n",
            "4       Lance Andersen  2004/10/18 20:03:08 +0100  2004/11/05 04:01:24 +0100   \n",
            "\n",
            "                assignee  ... Security  Performance Breakage commit_count  \\\n",
            "0                    A B  ...        0            0        0            0   \n",
            "1  Daniel John Debrunner  ...        0            0        0            1   \n",
            "2        Mamta A. Satoor  ...        0            0        0            1   \n",
            "3    Sunitha Kambhampati  ...        1            0        0            1   \n",
            "4     Myrna van Lunteren  ...        0            0        0            1   \n",
            "\n",
            "  file_count                                              Files Unnamed: 32  \\\n",
            "0          0                                                NaN         NaN   \n",
            "1          3  java/engine/org/apache/derby/iapi/services/mon...         NaN   \n",
            "2          1  java/engine/org/apache/derby/impl/sql/compile/...         NaN   \n",
            "3          1  java/engine/org/apache/derby/impl/services/jce...         NaN   \n",
            "4          4  java/engine/org/apache/derby/iapi/reference/SQ...         NaN   \n",
            "\n",
            "   Unnamed: 33  Unnamed: 34  Unnamed: 35  \n",
            "0          NaN          NaN          NaN  \n",
            "1          NaN          NaN          NaN  \n",
            "2          NaN          NaN          NaN  \n",
            "3          NaN          NaN          NaN  \n",
            "4          NaN          NaN          NaN  \n",
            "\n",
            "[5 rows x 36 columns]\n",
            "Cleaned and engineered dataset saved at: cleaned_engineered_dataset3.csv\n",
            "\n",
            "Sample of engineered features:\n",
            "   time_to_resolve  time_to_assign  time_to_fix  description_word_count  \\\n",
            "0      8473.900556        0.000000  6779.120444                       0   \n",
            "1      5766.705278     5722.272778  4613.364222                       0   \n",
            "2        58.105556       58.085556    46.484444                       0   \n",
            "3      5593.362500        0.000000  4474.690000                       0   \n",
            "4       550.593333      415.971111   440.474667                       0   \n",
            "\n",
            "   commenter_count  votes_watches_sum  priority_time_interaction  \n",
            "0                0                  0               16947.801111  \n",
            "1                0                  0               23066.821111  \n",
            "2                0                  0                 116.211111  \n",
            "3                0                  0               11186.725000  \n",
            "4                0                  0                1101.186667  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, BatchNormalization, Flatten, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = 'cleaned_engineered_dataset3.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Handling missing values (ensure no NaNs)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Convert categorical features to numerical using LabelEncoder\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "# Define target variable\n",
        "target = 'priority'\n",
        "y = df[target].values\n",
        "\n",
        "# Convert target to binary classification (e.g., median split)\n",
        "median_time = np.median(y)\n",
        "y_binary = (y >= median_time).astype(int)\n",
        "\n",
        "# Text processing for 'description' column (if available)\n",
        "if 'description' in df.columns:\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(df['description'].astype(str))\n",
        "    X_text = tokenizer.texts_to_sequences(df['description'].astype(str))\n",
        "    X_text = pad_sequences(X_text, maxlen=100)\n",
        "else:\n",
        "    X_text = np.zeros((len(df), 100))\n",
        "\n",
        "# Extract numerical and categorical features\n",
        "excluded_cols = ['description', target]\n",
        "feature_cols = [col for col in df.columns if col not in excluded_cols]\n",
        "X_tabular = df[feature_cols].values\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_tabular = scaler.fit_transform(X_tabular)\n",
        "\n",
        "# Split dataset\n",
        "X_text_train, X_text_test, X_tab_train, X_tab_test, y_train, y_test = train_test_split(\n",
        "    X_text, X_tabular, y_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Model architecture\n",
        "text_input = Input(shape=(100,), name='text_input')\n",
        "text_embedding = Embedding(input_dim=5000, output_dim=128, input_length=100)(text_input)\n",
        "text_lstm = LSTM(64, return_sequences=False)(text_embedding)\n",
        "text_dense = Dense(32, activation='relu')(text_lstm)\n",
        "\n",
        "# Tabular input\n",
        "tabular_input = Input(shape=(X_tab_train.shape[1],), name='tabular_input')\n",
        "tabular_dense = Dense(64, activation='relu')(tabular_input)\n",
        "tabular_dense = BatchNormalization()(tabular_dense)\n",
        "tabular_dense = Dropout(0.3)(tabular_dense)\n",
        "\n",
        "# Combine both inputs\n",
        "merged = Concatenate()([text_dense, tabular_dense])\n",
        "final_dense = Dense(64, activation='relu')(merged)\n",
        "final_dense = Dropout(0.3)(final_dense)\n",
        "output = Dense(1, activation='sigmoid', name='output')(final_dense)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=[text_input, tabular_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(\n",
        "    [X_text_train, X_tab_train], y_train,\n",
        "    validation_data=([X_text_test, X_tab_test], y_test),\n",
        "    epochs=20, batch_size=32\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model.save('/content/rnn_mlp_model1.h5')\n",
        "print('Model saved!')\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_probs = model.predict([X_text_test, X_tab_test])\n",
        "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred_probs)\n",
        "mae = mean_absolute_error(y_test, y_pred_probs)\n",
        "r2 = r2_score(y_test, y_pred_probs)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'R^2 Score: {r2}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjPo-4tbpI6O",
        "outputId": "af95ed0f-b0b4-4607-a2a7-6bc110704e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.7473 - loss: 0.5075 - val_accuracy: 0.9800 - val_loss: 0.0969\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9763 - loss: 0.1300 - val_accuracy: 0.9800 - val_loss: 0.1003\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9685 - loss: 0.1359 - val_accuracy: 0.9800 - val_loss: 0.0924\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9584 - loss: 0.1576 - val_accuracy: 0.9800 - val_loss: 0.0927\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9677 - loss: 0.1259 - val_accuracy: 0.9800 - val_loss: 0.0918\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9598 - loss: 0.1342 - val_accuracy: 0.9800 - val_loss: 0.0889\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9637 - loss: 0.1358 - val_accuracy: 0.9800 - val_loss: 0.0878\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9650 - loss: 0.1150 - val_accuracy: 0.9800 - val_loss: 0.0873\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9681 - loss: 0.1193 - val_accuracy: 0.9800 - val_loss: 0.0863\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9707 - loss: 0.0931 - val_accuracy: 0.9800 - val_loss: 0.0899\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9675 - loss: 0.1109 - val_accuracy: 0.9800 - val_loss: 0.0894\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9826 - loss: 0.0765 - val_accuracy: 0.9800 - val_loss: 0.0934\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9722 - loss: 0.0965 - val_accuracy: 0.9800 - val_loss: 0.0905\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9626 - loss: 0.1204 - val_accuracy: 0.9750 - val_loss: 0.0943\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9683 - loss: 0.0843 - val_accuracy: 0.9750 - val_loss: 0.0916\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9673 - loss: 0.0942 - val_accuracy: 0.9800 - val_loss: 0.0877\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9708 - loss: 0.1026 - val_accuracy: 0.9800 - val_loss: 0.0874\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9761 - loss: 0.0822 - val_accuracy: 0.9800 - val_loss: 0.0890\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9730 - loss: 0.0803 - val_accuracy: 0.9800 - val_loss: 0.0880\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9793 - loss: 0.0704 - val_accuracy: 0.9750 - val_loss: 0.0946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bacdc11a200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Mean Squared Error: 0.02087937854230404\n",
            "Mean Absolute Error: 0.04594748839735985\n",
            "R^2 Score: -0.06527435779571533\n",
            "Accuracy: 0.975\n",
            "Precision: 0.9798994974874372\n",
            "Recall: 0.9948979591836735\n",
            "F1 Score: 0.9873417721518988\n"
          ]
        }
      ]
    }
  ]
}